{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "REDWINE_PATH = \"../datasets/winequality-red.csv\"\n",
    "WHITEWINE_PATH = \"../datasets/winequality-white.csv\"\n",
    "\n",
    "# read red wine set of observations\n",
    "data_red = pd.read_csv(REDWINE_PATH,sep=',')\n",
    "data_red['color'] = 1 #redwine\n",
    "\n",
    "print(data_red.shape)\n",
    "\n",
    "# read white wine set of observations\n",
    "data_white = pd.read_csv(WHITEWINE_PATH,sep=',')\n",
    "data_white['color'] = 0 #whitewine\n",
    "\n",
    "print(data_white.shape)\n",
    "\n",
    "# merge the two sets in one\n",
    "data = data_red.merge(data_white, how='outer')\n",
    "fields = list(data.columns)\n",
    "print(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# show the histograms of values per feature (eg most of whines are at about 8 proof strength)\n",
    "sns.set()\n",
    "data.hist(figsize=(10,10),color='red', bins=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the historgram of wine rankings (quality between 1 and 10)\n",
    "data['quality'].hist(color='red', bins=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_red = pd.read_csv(REDWINE_PATH,sep=',')\n",
    "data_red['color'] = 1 #redwine\n",
    "\n",
    "print(data_red.shape)\n",
    "\n",
    "data_white = pd.read_csv(WHITEWINE_PATH,sep=',')\n",
    "data_white['color'] = 0 #whitewine\n",
    "\n",
    "print(data_white.shape)\n",
    "\n",
    "data = data_red.merge(data_white, how='outer')\n",
    "\n",
    "\n",
    "# based on the \"quality histograms\" above, we will drop the ratings with low counts (we will keep only 3 dimesnions = 5,6,7)\n",
    "data = data.drop(data[data.quality == 1].index)    # not recorded anyway\n",
    "data = data.drop(data[data.quality == 2].index)    # not recorded anyway\n",
    "data = data.drop(data[data.quality == 10].index)   # not recorded anyway\n",
    "data = data.drop(data[data.quality == 9].index)\n",
    "data = data.drop(data[data.quality == 3].index)\n",
    "data = data.drop(data[data.quality == 8].index)\n",
    "data = data.drop(data[data.quality == 4].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.quality.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = list(data.columns)\n",
    "print(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set in two: 1) color+features (observations)  2) quality (actuals)\n",
    "\n",
    "fields = list(data.columns[:-2])\n",
    "fields.append('color')  #adding color back\n",
    "X = data[fields]\n",
    "y = data['quality']\n",
    "print(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pearson correlation was used to identify which features correlate with wine quality. It looks as if higher the alcohol content the higher the quality. Lower density and volatile acidity also correlated with better quality as seen in the pairwise correlation chart the chart below. Only the top 5 correlated features were carried over to the KNN models."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = data[fields].corrwith(y)\n",
    "correlations.sort_values(inplace=True)\n",
    "\n",
    "# the following fields are the 5 retained as having the highest correlations to wine quality\n",
    "fields = correlations.map(abs).sort_values().iloc[-5:].index\n",
    "print(fields) #prints the top two abs correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# The figure below shows Pearson Pairwise correlation of features to wine quality.\n",
    "# Looks like alcohol and density are the most correlated with quality\n",
    "ax = correlations.plot(kind='bar')\n",
    "ax.set(ylim=[-1, 1], ylabel='pearson correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_MINMAXSCALER = False\n",
    "\n",
    "X = data[fields]\n",
    "if USE_MINMAXSCALER:\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "else:\n",
    "    # Alternative to previous scalar\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Fit on training set only.\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "X = pd.DataFrame(X, columns=['%s_scaled' % fld for fld in fields])\n",
    "print(X.columns) #scaled columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run Logistic Regression algorithm to create a \"prediction model\"\n",
    "\n",
    "Since the data captured is in different magnitude ranges, it is generally a good idea to scale so one feature doesnâ€™t get more influence than the other (in terms of scale)."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test_size: what proportion of original data is used for test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1/7.0, random_state=122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "MYSOLVER = \"lbfgs\" #\"newton-cg\"\n",
    "\n",
    "startproc = time.time()\n",
    "\n",
    "logreg = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='auto',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver=MYSOLVER,\n",
    "          tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "# fit the model with training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# predict the wine rankings for the test data set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "proctime = time.time() - startproc\n",
    "\n",
    "print(y_pred,X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# how did our model perform?\n",
    "count_misclassified = (y_test != y_pred).sum()\n",
    "print('Misclassified samples: {}'.format(count_misclassified))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") \n",
    "on a set of test data for which the true values are known.\n",
    "\n",
    "Here is a set of the most basic terms, which are whole numbers (not rates):\n",
    "1. true positives (TP): These are cases in which we predicted \"yes\" where it is actually \"yes\".\n",
    "2. true negatives (TN): We predicted no, and it is actually \"no\".\n",
    "3. false positives (FP): We predicted \"yes\", and was actually \"no\" (Also known as a \"Type I error.\"\n",
    "4. false negatives (FN): We predicted \"no\", and was actually \"yes\". (Also known as a \"Type II error.\")\n",
    "    \n",
    "This is a list of rates that are often computed from a confusion matrix for a binary classifier:\n",
    "\n",
    "**Accuracy**: Overall, how often is the classifier correct? \n",
    "    <br>&emsp;accuracy = (TP+TN)/total\n",
    "\n",
    "**Misclassification Rate**: Overall, how often is it wrong? \n",
    "    <br>&emsp;misrate = (FP+FN)/total\n",
    "    <br>&emsp;equivalent to 1 minus Accuracy\n",
    "    <br>&emsp;also known as \"Error Rate\"\n",
    "\n",
    "**True Positive Rate**: When it's actually yes, how often does it predict yes? \n",
    "    <br>&emsp;tprate = TP/actual_yes\n",
    "    <br>&emsp;also known as \"Sensitivity\" or \"Recall\"\n",
    "\n",
    "**False Positive Rate**: When it's actually no, how often does it predict yes?\n",
    "    <br>&emsp;fprate = FP/actual_no\n",
    "\n",
    "**True Negative Rate**: When it's actually no, how often does it predict no? \n",
    "    <br>&emsp;tnrate = TN/actual_no\n",
    "    <br>&emsp;equivalent to 1 minus False Positive Rate\n",
    "    <br>&emsp;also known as \"Specificity\"\n",
    "\n",
    "**Precision**: When it predicts yes, how often is it correct? \n",
    "    <br>&emsp;precision = TP/predicted_yes\n",
    "\n",
    "**Prevalence**: How often does the yes condition actually occur in our sample? \n",
    "    <br>&emsp;prevalence = actual_yes/total\n",
    "    \n",
    "![Confusion Matrix - Theoretical Foundation](https://raw.githubusercontent.com/antongeorgescu/machine-learning-documentation/master/images/Confusion-Matrix-2.PNG) "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "\n",
    "# Calculate the accuracy of prediction\n",
    "# Get the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.4f}'.format(accuracy))\n",
    "\n",
    "f1score = f1_score(y_test, y_pred, average='micro')\n",
    "print('F1_Score: {:.4f}'.format(f1score))\n",
    "\n",
    "# Get the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='.2g');\n",
    "plt.title('Confusion matrix of the KNN classifier')    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "summaryfile = 'ModelsFitness.txt'\n",
    "nbdir = os.getcwd()\n",
    "fsummary = open(f'{nbdir}\\\\{summaryfile}',\"a\") \n",
    "fsummary.write('Wine Quality Analysis with Logistic Regression\\tProcessing (sec):{:.4f}\\tAccuracy: {:.4f}\\tF1-Score: {:.4f}\\r\\n'.format(proctime,accuracy,f1score))\n",
    "fsummary.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}